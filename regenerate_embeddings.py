"""
Regeneraci√≥n INTELIGENTE de embeddings.
Usa DeepSeek para generar res√∫menes y detectar si hay informaci√≥n nueva.

Modos:
- python regenerate_embeddings.py           # Incremental (solo cambios)
- python regenerate_embeddings.py --full    # Regenerar todo
"""
import os
import sys
import logging
import time
import requests
from datetime import timedelta
from sqlalchemy import create_engine, text
from langchain_openai import OpenAIEmbeddings
from langchain_postgres import PGVector
from langchain_core.documents import Document

# Configurar logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Cargar .env si existe
if os.path.exists("mis_claves.env"):
    from dotenv import load_dotenv
    load_dotenv("mis_claves.env")

DATABASE_URL = os.getenv("DATABASE_URL")
COLLECTION_NAME = os.getenv("COLLECTION_NAME", "reviews_embeddings")
DISCORD_WEBHOOK_URL = os.getenv("DISCORD_WEBHOOK_URL")

# Importar utilidades
from db_utils import (
    get_connection, close_connection, migrate_embedding_columns,
    get_lugares_para_embedding, get_reviews_nuevas_sin_embedding,
    get_todas_reviews_lugar, actualizar_resumen_lugar
)
from deepseek_utils import generar_resumen_reviews, detectar_info_nueva, limpiar_texto


def get_sqlalchemy_url(url):
    if url and url.startswith("postgres://"):
        return url.replace("postgres://", "postgresql+psycopg2://", 1)
    elif url and url.startswith("postgresql://"):
        return url.replace("postgresql://", "postgresql+psycopg2://", 1)
    return url


def delete_embeddings_for_lugares(engine, nombres):
    """Elimina embeddings de lugares espec√≠ficos"""
    if not nombres:
        return 0
    
    nombres_escaped = [n.replace("'", "''") for n in nombres]
    nombres_str = "', '".join(nombres_escaped)
    
    query = f"""
        DELETE FROM langchain_pg_embedding 
        WHERE collection_id IN (
            SELECT uuid FROM langchain_pg_collection WHERE name = '{COLLECTION_NAME}'
        )
        AND cmetadata->>'nombre' IN ('{nombres_str}')
    """
    
    with engine.connect() as conn:
        result = conn.execute(text(query))
        conn.commit()
        return result.rowcount


def delete_all_embeddings(engine):
    """Elimina todos los embeddings de la colecci√≥n"""
    query = f"""
        DELETE FROM langchain_pg_embedding 
        WHERE collection_id IN (
            SELECT uuid FROM langchain_pg_collection WHERE name = '{COLLECTION_NAME}'
        )
    """
    with engine.connect() as conn:
        result = conn.execute(text(query))
        conn.commit()
        return result.rowcount


def create_document(lugar, resumen):
    """Crea un Document de LangChain para un lugar"""
    # Si es el mensaje gen√©rico de falta de info, NO generar embedding
    if not resumen or len(resumen.strip()) < 20 or "insuficiente informaci√≥n" in resumen:
        return None
    
    rating_raw = lugar.get('rating_gral', 0)
    try:
        if isinstance(rating_raw, str):
            rating_raw = rating_raw.replace(',', '.')
        rating = float(rating_raw) if rating_raw else 0.0
    except:
        rating = 0.0
    
    return Document(
        page_content=resumen,
        metadata={
            "nombre": str(lugar['nombre']),
            "rating": rating,
            "direccion": str(lugar.get('direccion', '') or ''),
            "zona": str(lugar.get('zona', '') or ''),
            "barrio": str(lugar.get('barrio', '') or ''),
            "categoria": str(lugar.get('categoria', '') or '')
        }
    )


def send_discord_report(stats):
    """Env√≠a reporte de ejecuci√≥n a Discord"""
    if not DISCORD_WEBHOOK_URL:
        return

    color = 0x00ff00 if stats['status'] == 'success' else 0xff0000
    
    mensaje = f"""**üß† QUE MORFAMOS - Regeneraci√≥n de Embeddings**
üìä **Tipo:** {stats['tipo']}
‚è±Ô∏è **Duraci√≥n:** {stats['duration']}

üìç **Lugares procesados:** {stats['lugares_procesados']}
üìù **Res√∫menes generados:** {stats['resumenes_generados']}
üöÄ **Embeddings creados:** {stats['embeddings_creados']}
"""
    
    payload = {
        "embeds": [{
            "description": mensaje,
            "color": color,
            "timestamp": datetime.now().isoformat() if 'datetime' in globals() else None
        }]
    }
    
    try:
        requests.post(DISCORD_WEBHOOK_URL, json=payload, timeout=10)
    except Exception as e:
        logger.error(f"Error enviando reporte a Discord: {e}")


def regenerate_full(resume=False):
    """Regenera TODOS los res√∫menes y embeddings desde cero"""
    start_time = time.time()
    logger.info(f"üîÑ Regeneraci√≥n COMPLETA de embeddings (Resume={resume})")
    
    # Migrar columnas si es necesario
    migrate_embedding_columns()
    
    engine = create_engine(get_sqlalchemy_url(DATABASE_URL))
    
    # Eliminar todos los embeddings existentes (correct even for resume, we rebuild the vector store)
    logger.info("üóëÔ∏è Eliminando embeddings existentes...")
    deleted = delete_all_embeddings(engine)
    logger.info(f"   Eliminados: {deleted}")
    
    # Obtener todos los lugares
    lugares = get_lugares_para_embedding()
    logger.info(f"üìç Lugares a procesar: {len(lugares)}")
    
    docs = []
    procesados = 0
    resumenes_count = 0
    skipped_count = 0
    
    limit_date = datetime.now() - timedelta(hours=24)
    
    for i, lugar in enumerate(lugares):
        nombre = lugar['nombre']
        
        # L√≥gica RESUME: Si ya se actualiz√≥ en las √∫ltimas 24hs, usamos lo que hay
        if resume and lugar.get('embedding_updated_at'):
             last_update = lugar['embedding_updated_at']
             # Asegurar que es datetime
             if isinstance(last_update, str):
                 try:
                     last_update = datetime.fromisoformat(last_update)
                 except:
                     pass
             
             if isinstance(last_update, datetime) and last_update > limit_date:
                 resumen = lugar.get('resumen_reviews')
                 if resumen and len(resumen) > 20:
                     logger.info(f"[{i+1}/{len(lugares)}] ‚è≠Ô∏è SKIPPING {nombre[:30]}... (Updated: {last_update})")
                     doc = create_document(lugar, resumen)
                     if doc:
                         docs.append(doc)
                         skipped_count += 1
                     continue

        # Obtener todas las rese√±as
        reviews = get_todas_reviews_lugar(nombre)
        
        if not reviews:
            continue
        
        # Generar resumen con DeepSeek
        logger.info(f"[{i+1}/{len(lugares)}] {nombre[:40]}... ({len(reviews)} reviews)")
        resumen = generar_resumen_reviews(reviews, nombre)
        
        if resumen:
            resumenes_count += 1
            # Guardar resumen en DB
            actualizar_resumen_lugar(nombre, resumen)
            
            # Crear documento para embedding
            doc = create_document(lugar, resumen)
            if doc:
                docs.append(doc)
        
        # Log de progreso
        if (i + 1) % 50 == 0:
            logger.info(f"   ‚è≥ Progreso: {i+1}/{len(lugares)}")
    
    # Generar embeddings
    embeddings_count = 0
    if docs:
        logger.info(f"üöÄ Generando embeddings para {len(docs)} lugares...")
        embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
        
        vectorstore = PGVector.from_documents(
            documents=docs,
            embedding=embeddings,
            connection=DATABASE_URL,
            collection_name=COLLECTION_NAME,
            use_jsonb=True
        )
        embeddings_count = len(docs)
        logger.info(f"‚úÖ {embeddings_count} embeddings generados!")
    else:
        logger.warning("‚ö†Ô∏è No hay documentos para generar embeddings")
    
    close_connection()
    
    # Enviar reporte
    duration = str(timedelta(seconds=int(time.time() - start_time)))
    send_discord_report({
        'status': 'success',
        'tipo': 'FULL (Manual)',
        'duration': duration,
        'lugares_procesados': len(lugares),
        'resumenes_generados': resumenes_count,
        'embeddings_creados': embeddings_count
    })


def regenerate_incremental():
    """Regenera solo los lugares que tienen informaci√≥n nueva"""
    start_time = time.time()
    logger.info("üîÑ Regeneraci√≥n INCREMENTAL de embeddings")
    
    # Migrar columnas si es necesario
    migrate_embedding_columns()
    
    engine = create_engine(get_sqlalchemy_url(DATABASE_URL))
    
    # Obtener todos los lugares con metadata de embedding
    lugares = get_lugares_para_embedding()
    logger.info(f"üìç Lugares en DB: {len(lugares)}")
    
    lugares_a_actualizar = []
    nuevos_resumenes = {}
    
    for i, lugar in enumerate(lugares):
        nombre = lugar['nombre']
        resumen_actual = lugar.get('resumen_reviews')
        embedding_date = lugar.get('embedding_updated_at')
        
        # Caso 1: No tiene resumen ‚Üí generar
        if not resumen_actual:
            reviews = get_todas_reviews_lugar(nombre)
            if reviews:
                logger.info(f"[NUEVO] {nombre[:40]}...")
                resumen = generar_resumen_reviews(reviews, nombre)
                if resumen:
                    nuevos_resumenes[nombre] = resumen
                    lugares_a_actualizar.append(lugar)
            continue
        
        # Caso 2: Tiene resumen ‚Üí verificar si hay reviews nuevas QUALIFICADAS
        reviews_nuevas = get_reviews_nuevas_sin_embedding(nombre, embedding_date)
        
        if not reviews_nuevas:
            continue
        
        # Filtro de CALIDAD: > 30 caracteres (despu√©s de limpiar repetidos/puntuaci√≥n)
        reviews_validas = [r for r in reviews_nuevas if r and len(limpiar_texto(str(r))) > 30]
        
        # Umbral M√çNIMO de cantidad: al menos 20 reviews nuevas v√°lidas para justificar an√°lisis
        # (Optimizaci√≥n de costos para estudiante: solo regenerar cuando hay mucho volumen nuevo)
        if len(reviews_validas) < 20:
            # Si hay pocas reviews nuevas, solo actualizamos la fecha para no chequear ma√±ana lo mismo
            # (A menos que pasaran > 30 d√≠as, eso se podr√≠a agregar luego)
            if reviews_nuevas: # Si hay reviews pero son cortas o pocas
                 actualizar_resumen_lugar(nombre, resumen_actual) # Actualiza timestamp sin cambiar resumen
            continue

        logger.info(f"[CHECK] {nombre[:40]}... ({len(reviews_validas)} reviews nuevas v√°lidas)")
        
        # Verificar si aportan info nueva con DeepSeek
        if detectar_info_nueva(resumen_actual, reviews_validas):
            # Regenerar resumen completo
            todas_reviews = get_todas_reviews_lugar(nombre) # Devuelve dicts con rating
            resumen = generar_resumen_reviews(todas_reviews, nombre)
            if resumen:
                nuevos_resumenes[nombre] = resumen
                lugares_a_actualizar.append(lugar)
                logger.info(f"   ‚úÖ Info nueva detectada, regenerando")
        else:
            logger.info(f"   ‚è≠Ô∏è Sin info nueva relevante, actualizando solo timestamp")
            actualizar_resumen_lugar(nombre, resumen_actual) # Actualiza solo fecha
    
    logger.info(f"\nüìä Lugares a actualizar: {len(lugares_a_actualizar)}")
    
    # Enviar reporte si NO hubo cambios (para saber que corri√≥)
    if not lugares_a_actualizar:
        logger.info("‚úÖ Todo est√° actualizado, no hay cambios necesarios")
        close_connection()
        duration = str(timedelta(seconds=int(time.time() - start_time)))
        send_discord_report({
            'status': 'success',
            'tipo': 'Incremental (Sin cambios)',
            'duration': duration,
            'lugares_procesados': len(lugares),
            'resumenes_generados': 0,
            'embeddings_creados': 0
        })
        return
    
    # Actualizar res√∫menes en DB
    logger.info("üíæ Guardando res√∫menes en DB...")
    for nombre, resumen in nuevos_resumenes.items():
        actualizar_resumen_lugar(nombre, resumen)
    
    # Eliminar embeddings viejos de los lugares a actualizar
    nombres_actualizar = [l['nombre'] for l in lugares_a_actualizar]
    logger.info(f"üóëÔ∏è Eliminando embeddings viejos de {len(nombres_actualizar)} lugares...")
    delete_embeddings_for_lugares(engine, nombres_actualizar)
    
    # Crear nuevos documentos
    docs = []
    for lugar in lugares_a_actualizar:
        nombre = lugar['nombre']
        resumen = nuevos_resumenes.get(nombre)
        if resumen:
            doc = create_document(lugar, resumen)
            if doc:
                docs.append(doc)
    
    # Generar embeddings
    if docs:
        logger.info(f"üöÄ Generando {len(docs)} embeddings nuevos...")
        embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
        
        vectorstore = PGVector.from_documents(
            documents=docs,
            embedding=embeddings,
            connection=DATABASE_URL,
            collection_name=COLLECTION_NAME,
            use_jsonb=True
        )
        
        logger.info(f"‚úÖ {len(docs)} embeddings actualizados!")
    
    close_connection()
    
    # Reporte final con cambios
    duration = str(timedelta(seconds=int(time.time() - start_time)))
    send_discord_report({
        'status': 'success',
        'tipo': 'Incremental (Con actualizaciones)',
        'duration': duration,
        'lugares_procesados': len(lugares),
        'resumenes_generados': len(nuevos_resumenes),
        'embeddings_creados': len(docs)
    })


def regenerate_embeddings_only():
    """Solo regenera embeddings usando los res√∫menes YA GUARDADOS en DB"""
    start_time = time.time()
    logger.info("üß© Regeneraci√≥n SOLO de embeddings (Desde DB existente)")
    
    # Migrar columnas si es necesario
    migrate_embedding_columns()
    engine = create_engine(get_sqlalchemy_url(DATABASE_URL))
    
    # Obtener lugares que YA tienen resumen
    conn = get_connection()
    cursor = conn.cursor()
    cursor.execute("SELECT nombre, rating_gral, direccion, zona, barrio, categoria, resumen_reviews FROM lugares WHERE resumen_reviews IS NOT NULL AND length(resumen_reviews) > 20")
    rows = cursor.fetchall()
    
    lugares_con_resumen = []
    for r in rows:
        lugares_con_resumen.append({
            'nombre': r[0],
            'rating_gral': r[1],
            'direccion': r[2],
            'zona': r[3],
            'barrio': r[4],
            'categoria': r[5],
            'resumen_reviews': r[6]
        })
    conn.close()
    
    logger.info(f"üìç Lugares con resumen apto: {len(lugares_con_resumen)}")
    
    if not lugares_con_resumen:
        logger.warning("No hay lugares con resumen para procesar.")
        return

    # Eliminar todos los embeddings existentes para empezar limpio
    logger.info("üóëÔ∏è Eliminando embeddings existentes...")
    delete_all_embeddings(engine)
    
    # Crear documentos
    docs = []
    for l in lugares_con_resumen:
        doc = create_document(l, l['resumen_reviews'])
        if doc:
            docs.append(doc)
            
    # Generar embeddings
    embeddings_count = 0
    if docs:
        logger.info(f"üöÄ Generando embeddings para {len(docs)} lugares...")
        embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
        
        vectorstore = PGVector.from_documents(
            documents=docs,
            embedding=embeddings,
            connection=DATABASE_URL,
            collection_name=COLLECTION_NAME,
            use_jsonb=True
        )
        embeddings_count = len(docs)
        logger.info(f"‚úÖ {embeddings_count} embeddings generados!")
    
    close_connection()
    
    duration = str(timedelta(seconds=int(time.time() - start_time)))
    send_discord_report({
        'status': 'success',
        'tipo': 'Embeddings Only (Recuperaci√≥n)',
        'duration': duration,
        'lugares_procesados': len(lugares_con_resumen),
        'resumenes_generados': 0,
        'embeddings_creados': embeddings_count
    })


if __name__ == "__main__":
    from datetime import datetime
    if "--full" in sys.argv:
        resume = "--resume" in sys.argv
        regenerate_full(resume=resume)
    elif "--embed-only" in sys.argv:
        regenerate_embeddings_only()
    else:
        regenerate_incremental()
